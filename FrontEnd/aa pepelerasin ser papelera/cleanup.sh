#!/bin/bash

echo "ðŸ§¹ Limpiando despliegue de Kubernetes..."

# Colores para output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m' # No Color

# ConfiguraciÃ³n de variables
DOCKER_USER="pablo03r"

info_msg() {
    echo -e "${YELLOW}â„¹ï¸  $1${NC}"
}

success_msg() {
    echo -e "${GREEN}âœ… $1${NC}"
}

error_msg() {
    echo -e "${RED}âŒ $1${NC}"
}

important_msg() {
    echo -e "${BLUE}ðŸ”µ $1${NC}"
}

ingress_msg() {
    echo -e "${PURPLE}ðŸŒ $1${NC}"
}

# FunciÃ³n para verificar si un comando existe
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

echo "ðŸ—‚ï¸  Estado actual del cluster:"
echo "================================"
kubectl get all -n so1-fase2 2>/dev/null || echo "Namespace so1-fase2 no existe o estÃ¡ vacÃ­o"
echo ""

# Mostrar estado de Ingress antes de limpiar
ingress_msg "Estado actual de Ingress:"
echo "=========================="
kubectl get ingress -n so1-fase2 2>/dev/null || echo "No hay recursos de Ingress en so1-fase2"
echo ""

# ========================================
# LIMPIEZA DE INGRESS
# ========================================
ingress_msg "Limpiando recursos de Ingress..."

# Lista de todos los posibles ingress que hemos creado
INGRESS_RESOURCES=(
    "python-api-ingress"
    "nodejs-api-ingress-canary"
    "nodejs-api-direct-ingress"
    "metricas-python-ingress"
    "metricas-nodejs-ingress-canary"
    "monitoring-api-ingress"
    "nodejs-api-ingress"
)

for ingress in "${INGRESS_RESOURCES[@]}"; do
    if kubectl get ingress "$ingress" -n so1-fase2 >/dev/null 2>&1; then
        kubectl delete ingress "$ingress" -n so1-fase2 2>/dev/null || true
        success_msg "Ingress $ingress eliminado"
    else
        info_msg "Ingress $ingress no existe (OK)"
    fi
done

# Eliminar cualquier ingress restante en el namespace
info_msg "Eliminando cualquier Ingress restante en so1-fase2..."
kubectl delete ingress --all -n so1-fase2 2>/dev/null || true

# ========================================
# LIMPIEZA DE LOAD BALANCER PERSONALIZADO
# ========================================
ingress_msg "Limpiando Load Balancer personalizado..."

# Eliminar deployment y service del load balancer NGINX personalizado
kubectl delete deployment load-balancer -n so1-fase2 2>/dev/null && success_msg "Load Balancer deployment eliminado" || info_msg "Load Balancer deployment no existe"
kubectl delete service load-balancer-service -n so1-fase2 2>/dev/null && success_msg "Load Balancer service eliminado" || info_msg "Load Balancer service no existe"
kubectl delete configmap nginx-lb-config -n so1-fase2 2>/dev/null && success_msg "Load Balancer ConfigMap eliminado" || info_msg "Load Balancer ConfigMap no existe"
kubectl delete configmap debug-scripts -n so1-fase2 2>/dev/null && success_msg "Debug scripts ConfigMap eliminado" || info_msg "Debug scripts ConfigMap no existe"

# Eliminar NetworkPolicies si existen
kubectl delete networkpolicy allow-load-balancer-to-apis -n so1-fase2 2>/dev/null && success_msg "NetworkPolicy eliminada" || info_msg "NetworkPolicy no existe"

# ========================================
# LIMPIEZA ORIGINAL
# ========================================
# Eliminar deployments y services especÃ­ficos
info_msg "Eliminando recursos especÃ­ficos de APIs..."

if [ -f "k8s/python-api.yaml" ]; then
    kubectl delete -f k8s/python-api.yaml 2>/dev/null || true
    success_msg "Recursos de Python API eliminados"
else
    error_msg "Archivo k8s/python-api.yaml no encontrado"
fi

if [ -f "k8s/nodejs-api.yaml" ]; then
    kubectl delete -f k8s/nodejs-api.yaml 2>/dev/null || true
    success_msg "Recursos de Node.js API eliminados"
else
    error_msg "Archivo k8s/nodejs-api.yaml no encontrado"
fi

if [ -f "k8s/postgres.yaml" ]; then
    kubectl delete -f k8s/postgres.yaml 2>/dev/null || true
    success_msg "Recursos de PostgreSQL eliminados"
else
    error_msg "Archivo k8s/postgres.yaml no encontrado"
fi

# Eliminar archivos de configuraciÃ³n si existen
if [ -f "canary-traffic-split.yaml" ]; then
    kubectl delete -f canary-traffic-split.yaml 2>/dev/null || true
    success_msg "ConfiguraciÃ³n Canary eliminada"
fi

if [ -f "loadbalancer-fix.yaml" ]; then
    kubectl delete -f loadbalancer-fix.yaml 2>/dev/null || true
    success_msg "ConfiguraciÃ³n Load Balancer eliminada"
fi

# Forzar eliminaciÃ³n de pods si estÃ¡n en estado Terminating
info_msg "Verificando pods en estado Terminating..."
TERMINATING_PODS=$(kubectl get pods -n so1-fase2 --field-selector=status.phase=Terminating --no-headers 2>/dev/null | awk '{print $1}')
if [ ! -z "$TERMINATING_PODS" ]; then
    info_msg "Forzando eliminaciÃ³n de pods en estado Terminating..."
    echo "$TERMINATING_PODS" | xargs -r kubectl delete pod --force --grace-period=0 -n so1-fase2
fi

# Eliminar PersistentVolumeClaims (antes del namespace)
info_msg "Eliminando PersistentVolumeClaims..."
kubectl delete pvc --all -n so1-fase2 2>/dev/null || true

# Eliminar namespace (esto eliminarÃ¡ todo lo que estÃ© dentro)
info_msg "Eliminando namespace so1-fase2..."
kubectl delete namespace so1-fase2 2>/dev/null || true

# Esperar a que el namespace se elimine completamente
info_msg "Esperando a que el namespace se elimine completamente..."
timeout 60s bash -c 'while kubectl get namespace so1-fase2 2>/dev/null; do sleep 2; done' || true

# Eliminar PersistentVolumes (estÃ¡n fuera del namespace)
info_msg "Eliminando PersistentVolumes..."
kubectl delete pv postgres-pv 2>/dev/null || true
kubectl get pv | grep "so1-fase2" | awk '{print $1}' | xargs -r kubectl delete pv 2>/dev/null || true

# Limpiar recursos huÃ©rfanos
info_msg "Limpiando posibles recursos huÃ©rfanos..."
kubectl delete configmaps,secrets -l app=nodejs-api --all-namespaces 2>/dev/null || true
kubectl delete configmaps,secrets -l app=python-api --all-namespaces 2>/dev/null || true
kubectl delete configmaps,secrets -l app=postgres --all-namespaces 2>/dev/null || true
kubectl delete configmaps,secrets -l app=load-balancer --all-namespaces 2>/dev/null || true

# ========================================
# VERIFICAR INGRESS CONTROLLER
# ========================================
ingress_msg "Verificando estado del NGINX Ingress Controller..."
if kubectl get pods -n ingress-nginx >/dev/null 2>&1; then
    echo "ðŸ“Š Pods del Ingress Controller:"
    kubectl get pods -n ingress-nginx -o wide | grep -E "(NAME|nginx-controller)" || echo "No hay pods del controller corriendo"
    echo ""
    
    # Mostrar logs recientes del ingress controller para debug
    info_msg "Ãšltimas 5 lÃ­neas de logs del Ingress Controller:"
    kubectl logs -n ingress-nginx -l app.kubernetes.io/component=controller --tail=5 2>/dev/null || echo "No se pudieron obtener logs"
    echo ""
else
    important_msg "NGINX Ingress Controller no estÃ¡ instalado o no estÃ¡ en ingress-nginx namespace"
fi

# Limpiar imÃ¡genes de Minikube si estÃ¡ disponible (IMÃGENES ACTUALIZADAS)
if command_exists minikube && minikube status >/dev/null 2>&1; then
    important_msg "Detectado Minikube en ejecuciÃ³n"
    read -p "Â¿Deseas eliminar las imÃ¡genes de Minikube? (y/N): " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        info_msg "Eliminando imÃ¡genes de Minikube..."
        # ImÃ¡genes antiguas (por compatibilidad)
        minikube image rm nodejs-monitoring-api:latest 2>/dev/null || true
        minikube image rm python-monitoring-api:latest 2>/dev/null || true
        # Nuevas imÃ¡genes de Docker Hub
        minikube image rm ${DOCKER_USER}/api1-nodejs-fase2:latest 2>/dev/null || true
        minikube image rm ${DOCKER_USER}/api1-python-fase2:latest 2>/dev/null || true
        success_msg "ImÃ¡genes eliminadas de Minikube"
    fi
fi

# Limpiar imÃ¡genes Docker locales (IMÃGENES ACTUALIZADAS)
read -p "Â¿Deseas eliminar las imÃ¡genes Docker locales? (y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    info_msg "Eliminando imÃ¡genes Docker locales..."
    # ImÃ¡genes antiguas
    docker rmi nodejs-monitoring-api:latest 2>/dev/null || true
    docker rmi python-monitoring-api:latest 2>/dev/null || true
    # Nuevas imÃ¡genes de Docker Hub
    docker rmi ${DOCKER_USER}/api1-nodejs-fase2:latest 2>/dev/null || true
    docker rmi ${DOCKER_USER}/api1-python-fase2:latest 2>/dev/null || true
    
    # Limpiar imÃ¡genes dangling
    info_msg "Limpiando imÃ¡genes Docker sin etiqueta..."
    docker image prune -f 2>/dev/null || true
    
    success_msg "ImÃ¡genes Docker eliminadas"
fi

# Limpieza adicional opcional
read -p "Â¿Deseas hacer una limpieza profunda de Docker? (contenedores parados, redes, volÃºmenes) (y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    info_msg "Realizando limpieza profunda de Docker..."
    docker container prune -f 2>/dev/null || true
    docker network prune -f 2>/dev/null || true
    docker volume prune -f 2>/dev/null || true
    success_msg "Limpieza profunda de Docker completada"
fi

echo ""
echo "ðŸŽ‰ Limpieza completada exitosamente!"
echo
